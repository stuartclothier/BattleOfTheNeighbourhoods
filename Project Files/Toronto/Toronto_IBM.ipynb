{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-racing",
   "metadata": {},
   "source": [
    "# Clustering Toronto Neighbourhoods\n",
    "#### Part 1: Data preperation\n",
    "\n",
    "This notebook extracts Neighbourhood information from [Wikipedia](https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M) and combines it with geographical coordinates, obtained from [cocl.us](https://cocl.us/Geospatial_data). Neighbourhoods from Downtown, North, East and Central Toronto are visualised using Folium and saved to csv for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-links",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Built in libraries\n",
    "import requests # HTTP requests\n",
    "\n",
    "# Third party libraries\n",
    "import numpy as np # arrays \n",
    "import pandas as pd # Data structures\n",
    "\n",
    "import folium # Visualising interactive maps\n",
    "\n",
    "import matplotlib.pyplot as plt # Plotting simple maps\n",
    "import matplotlib.cm as cm # Colourmaps\n",
    "import matplotlib.colors as colors # converting colours to RGB\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # Min Max Scaling for features\n",
    "\n",
    "from sklearn.cluster import KMeans # KMeans clustering model\n",
    "\n",
    "from sklearn.metrics import silhouette_score # silhouette score used for helping determine K \n",
    "from yellowbrick.cluster import SilhouetteVisualizer # Creating Silhouette plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-theme",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Here, the geographical information created in part 1 and the venue categories information created in part 2 are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "equipped-gilbert",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tor_boro.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d21f574b5c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Toronto Neighbourhoods geographical information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtor_boro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tor_boro.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Count of venue catgeories within 500, 1000 and 2000m radius of Toronto Neighbourhoods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Strored in a dict for ease of use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.julia\\conda\\3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tor_boro.csv'"
     ]
    }
   ],
   "source": [
    "# Toronto Neighbourhoods geographical information\n",
    "tor_boro = pd.read_csv('tor_boro.csv')  \n",
    "\n",
    "# Count of venue catgeories within 500, 1000 and 2000m radius of Toronto Neighbourhoods\n",
    "# Strored in a dict for ease of use\n",
    "R = [500, 1000, 2000]\n",
    "toronto_venues = {r:pd.read_csv('toronto_venues_'+str(r)+'.csv') for r in R}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-tension",
   "metadata": {},
   "source": [
    "## Analysing Neighbourhoods\n",
    "\n",
    "Now that we have the 39 Toronto neighbourhoods with a count of nearby venues, grouped by category, we can proceed to cluster the Neighbourhoods.\n",
    "\n",
    "First we scale the venue category counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = {r:MinMaxScaler().fit_transform(toronto_venues[r][list(toronto_venues[r].columns.values)[1:]]) for r in R}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-screw",
   "metadata": {},
   "source": [
    "## Silhouette Coefficients\n",
    "\n",
    "The higher the value of the silhouette coefficient or score, the better each data points 'fits' to its cluster and 'does not fit' to the other clusters (i.e. similarity within cluster/against dissimilarity to other clusters).\n",
    "\n",
    "The sillhoutte coefficient calclulated below is the mean of the silhouette coefficient or score of each data point. Therefore, these plots are a type of 'goodness of fit test' of the overall kmeans model. \n",
    "\n",
    "Sillhoutte coefficients take values between -1 and 1. For an data point with a score of 1, it would indicate optimal clustering. A value of 0 indicates that observation lies exactly between clusters, while a negative values indicates it is likely in the wrong cluster. \n",
    "\n",
    "For more information see the sci-kit learn [docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range of k values tested for\n",
    "K = range(2,16)\n",
    "\n",
    "# Create a list of K-Means cluster algorithms for the scaled vlaues over the range of ks\n",
    "kmeans = {r:[KMeans(n_clusters=k, random_state=0).fit(scaled_features[r]) for k in K] for r in R}\n",
    "\n",
    "# Create a list of silhoutte scores for the K-Means cluster algorithms over the range of ks\n",
    "sil = {r:[silhouette_score(scaled_features[r], model.predict(scaled_features[r]))\n",
    "          for model in kmeans[r]] for r in R}\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,15)\n",
    "\n",
    "ax1 = plt.subplot(311)\n",
    "ax1.set_title('Silhouette coefficients for search radius of 500m')\n",
    "ax1.scatter(K,sil[500])\n",
    "\n",
    "ax2 = plt.subplot(312)\n",
    "ax2.set_title('Silhouette coefficients for search radius of 1000m')\n",
    "ax2.scatter(K,sil[1000])\n",
    "\n",
    "ax3 = plt.subplot(313)\n",
    "ax3.set_title('Silhouette coefficients for search radius of 2000m')\n",
    "ax3.scatter(K,sil[2000])\n",
    "\n",
    "for ax in (ax1,ax2,ax3):\n",
    "    ax.set(xlabel='K clusters', ylabel='Silhouette score',xticks=K)\n",
    "    ax.grid(which='minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-infection",
   "metadata": {},
   "source": [
    "From the above plots we can see the closeness of fit for the three search radius for a range of 2 to 15 clusters. \n",
    "\n",
    "As noted above, the higher the value of K, the better the data points fit the clusters. \n",
    "\n",
    "For 500 metres a 2 cluster solution has the best fit. In fact, this is the best fit of all of the models. We also note that for this search radius, we have relatively high silhouette coefficients for values of K up to 10. The high coefficients for this search radius are possibly due to the Neighbourhoods being less likely to overalap search areas and share venues, therefore being more diverse and easy to cluster. \n",
    "\n",
    "For 1000 metres, 3 to 5 clusters appears to be optimal. However, we do note that these values are quite low compared to other radii.  \n",
    "\n",
    "In the 2000m plot we see again the 2 clusters yields the highest coefficient score. 3 and 4 cluster solutions also appear to be relatively strong, with a sharp drop off after this point.\n",
    "\n",
    "As the silhouette coefficient of the model is simply the mean of the silhouette coefficients of all the individual data points, it may be skewed based on some extremely well or poorly fitted data points.\n",
    "\n",
    "We may have a high silhouette coefficient for a model as a result of a majority certain clusters or data points fitting really well together, while some clusters or data points fit really poorly. \n",
    "\n",
    "Silhouette plots can be used to visualise the silhouette coefficients of all individual data points, as well as the cluster sizes.\n",
    "\n",
    "Ideally, we want clusters of similar sizes, with the majority of data points fitting consitently well. \n",
    "\n",
    "Below we have produced Silhouette plots of some relvant values of K for our search radii. The plots have also been saved as .png files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-reception",
   "metadata": {},
   "source": [
    "## Silhouette Plots\n",
    "\n",
    "### 500m search radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2,8)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "for k in K:\n",
    "    fig = plt.figure()\n",
    "    visualizers = SilhouetteVisualizer(KMeans(n_clusters=k, random_state=0), colors='yellowbrick')\n",
    "    visualizers.fit(scaled_features[500])\n",
    "    visualizers.show(outpath=\"silhouette_plots/500m/silhouette_plot_R=500m_&_K=\"+str(k)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-prince",
   "metadata": {},
   "source": [
    "### 1000m search radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(3,6)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "for k in K:\n",
    "    fig = plt.figure()\n",
    "    visualizers = SilhouetteVisualizer(KMeans(n_clusters=k, random_state=0), colors='yellowbrick')\n",
    "    visualizers.fit(scaled_features[1000])\n",
    "    visualizers.show(outpath=\"silhouette_plots/1000m/silhouette_plot_R=1000m_&_K=\"+str(k)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-reading",
   "metadata": {},
   "source": [
    "### 2000m search radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2,5)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "for k in K:\n",
    "    fig = plt.figure()\n",
    "    visualizers = SilhouetteVisualizer(KMeans(n_clusters=k, random_state=0), colors='yellowbrick')\n",
    "    visualizers.fit(scaled_features[2000])\n",
    "    visualizers.show(outpath=\"silhouette_plots/2000m/silhouette_plot_R=2000m_&_K=\"+str(k)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-instruction",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-welsh",
   "metadata": {},
   "source": [
    "The Silhouette plots for the 500 metre search radius did not yield particularly encouraging results. We notice the high mean value is a result of the similarity of a large section of these data points. This cluster is represented by the thickest and longest spike, which can be seen and remains significant in nearly all of the 500 metre plots. Further investigation of this shows that this is a result of the outer Toronto having very few search results. This can be seen in the Mapping Clusters and conclusion section.\n",
    "\n",
    "Despite the coefficient bias to the outer cluster, we note that for this radius 2 clusters still appears to be the most optimal solution. We should also note that from 5 clusters and above, outlier clusters and small clusters of 2 or 3 neighbourhoods begin to become common, meaning these solutions possibly aren't ideal either. \n",
    "\n",
    "Given a 1000 metre search radius the data appears to be dispersed somewhat better as we have yielded clusters of more consistant sizes and 'goodness of fit' values. However, there are still some data points which show very little correlation to their clusters and the overall coefficient mean is far from ideal. The 4 cluster solution seems like the best fit with no real outliers and good evenly dispersed clusters.\n",
    "\n",
    "The silhouette plots for the 2000 metre search radius confirm that increasing the number of clusters above 2 only results in less well fitting clusters, as suggested by the mean silhouette coefficient values. Similar to the 500m radius clusters, this radius appears to benifit from an increased coeffiecient mean due to the similarity of one set of data points. Further inspections of these points shows that these are the inner Neighbourhoods (see the map in part 4). This is predictable as many of these neighbourhoods are within the search radius of each other and will likely share nearby venues in the result set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-martial",
   "metadata": {},
   "source": [
    "## Part 4: Clustering and visualising the neighbourhoods\n",
    "\n",
    "This notebook clusters toronto neighbourhoods based on the similarity of nearby venue categorys and visualises them using folium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-encounter",
   "metadata": {},
   "source": [
    "## Define function\n",
    "\n",
    "mapClusters is used for visualisation of clustered neighbourhoods, produced with folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapClusters(k: int,r: int) -> folium.Map: \n",
    "    '''\n",
    "    This function uses folium to map clustered neighbourhoods, which are distinguished\n",
    "    by colour.\n",
    "    \n",
    "    Before running this function, kmeans clustering must be run on the toronto\n",
    "    venues (scaled) data, which is obtained from the getNearbyVenueCats function.\n",
    "    \n",
    "    The input k refers to the number of clusters which was used, while n is the\n",
    "    radius used in the getNearbyVenueCats function.   \n",
    "    '''\n",
    "    \n",
    "    toronto_clusters = pd.DataFrame(toronto_venues[r].Neighbourhood).merge(\n",
    "        tor_boro[['Neighbourhood', 'Latitude', 'Longitude']])\n",
    "\n",
    "    #Get central coordinates for Toronto map\n",
    "    latitude = tor_boro.Latitude.mean()\n",
    "    longitude = tor_boro.Longitude.mean()\n",
    "\n",
    "    # create map\n",
    "    map_clusters = folium.Map(location=[latitude, longitude], zoom_start=12)\n",
    "\n",
    "    # set color scheme for the clusters\n",
    "    x = np.arange(k)\n",
    "    ys = [i + x + (i*x)**2 for i in range(k)]\n",
    "    colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(scaled_features[r])\n",
    "    \n",
    "    # add markers to the map\n",
    "    markers_colors = []\n",
    "    for lat, lon, poi, cluster in zip(toronto_clusters['Latitude'], toronto_clusters['Longitude'],\n",
    "                                      toronto_clusters['Neighbourhood'], kmeans.labels_):\n",
    "        label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "        folium.CircleMarker(\n",
    "            [lat, lon],\n",
    "            radius=5,\n",
    "            popup=label,\n",
    "            color=rainbow[cluster-1],\n",
    "            fill=True,\n",
    "            fill_color=rainbow[cluster-1],\n",
    "            fill_opacity=0.7).add_to(map_clusters)\n",
    "\n",
    "    return map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-breakdown",
   "metadata": {},
   "source": [
    "## Analysing Neighbourhoods\n",
    "\n",
    "Now that we have the 39 Toronto neighbourhoods with a count of nearby venues, grouped by category, we can proceed to cluster the Neighbourhoods.\n",
    "\n",
    "First we scale the venue category counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = {r:MinMaxScaler().fit_transform(toronto_venues[r][list(toronto_venues[r].columns.values)[1:]]) for r in R}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-procurement",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means clustering will be used to cluster neighbourhoods based on the similarity of the number of venue categories nearby.\n",
    "\n",
    "Silhouette coefficients for K-means models given various values of K, as well as their respective silhouette plots, were used to determine which combinations of radii and clusters should be investigated further. This can be seen in part 3 of the project.\n",
    "\n",
    "The k means models are created and fitted to the scaled data in the mapClusters function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-scholarship",
   "metadata": {},
   "source": [
    "## Mapping clusters\n",
    "\n",
    "The mapClusters function was used to produce maps of the Toronto neighbourhoods given a specified number of clusters and search radius.\n",
    "\n",
    "#### 500m, 2 cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters and radius\n",
    "k = 2\n",
    "r = 500\n",
    "\n",
    "mapClusters(k,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-bunny",
   "metadata": {},
   "source": [
    "#### 2000m, 2 cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters and radius\n",
    "k = 2\n",
    "r = 2000\n",
    "\n",
    "mapClusters(k,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-cricket",
   "metadata": {},
   "source": [
    "#### 1000m, 4 cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters and radius\n",
    "k = 4\n",
    "r = 1000\n",
    "\n",
    "mapClusters(k,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-marking",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "#### 2 Clusters\n",
    "\n",
    "As seen in part 3, the 500m radius and 2 cluster solution had the highest mean silhouette score, which indicates the best closeness of fit. \n",
    "\n",
    "Similarly, for 2000m, the 2 cluster solution had the highest silhouette coefficient for that search radius.\n",
    "\n",
    "Although, these results do have relatively well defined clusters, a 2 cluster solution does not really give us much useful insight. As can be seen in the visualisations, with 2 clusters, we see that neighbourhoods are simply split between those close to the centre and those further out. It is quite intuiative that neighbouroods nearer to a city centre will have more venues nearby than a neighbourhood further away from the centre, and perhaps using kmeans clustering to confirm this is a little superfluous.\n",
    "\n",
    "In fact, the only difference between the 500 and 2000m clusters is that the centre cluster has a larger radius for the larger search radius case, again, as we would expect. \n",
    "\n",
    "We should however note that these clusters aren't completely geographically spherical. This proves that, even close to the city centre, some neighbourhoods have a higher density of venues than others and that this density of venues doesn't decline exactly evenly proportionally as the distance increases.\n",
    "\n",
    "#### 4 Clusters\n",
    "\n",
    "The 1000m 4 cluster solution has produced a somewhat less predictable output. The outer neighbourhoods have been split somewhat randomly while the central neighbourhoods appear to have been split into a group nearer to the university and a group nearer to the train station.\n",
    "\n",
    "We should note however that this has the lowest silhouette score of those shown above meaning that these clusters are possibly not as optimal as some of the other solutions. If we were to investigate some of the data points within these clusters we would in fact find that some neighbourhoods within clusters are not particularly similar. Checking the silhouette plot from part 3, this is particulary evident with clusters 2 and 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-tribe",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "While we have gained some insights into what types of venues are nearby to the neighbourhoods around Toronto, it is worth noting that there are some limitations to this method and the solutions obtained.\n",
    "\n",
    "Firstly, the data is not organised very clustered manner. The mean silhouette scores are not particularly high while the silhouette plots showed that it was hard to evenly group neighbourhoods in similarly sized and distanced (in the feature space) clusters.  \n",
    "\n",
    "One issue with the dataset may be the 100 result limit forced by Foursquare. It is highly likely that most of the inner Toronto neighbourhoods had many venues missing, potentially skewing results.\n",
    "\n",
    "Another issue that could be addressed in future test would be feature selection. The prescence of a very small number of 'College & University', 'Professional & Other Places' and 'Residence' venues in very few neighbourhoods, made it very hard to properly cluster these neighbourhoods. Removing these categories it was found that the mean silhouette score increased in most cases and could even be increased beyond 0.50.\n",
    "\n",
    "Including more feature, such as crime rate, public transport accessabilty, average purchase/renatal cost, nearby schools and so on, may also discover more similarities and disimilarites that could be useful for accurate clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-leave",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
